{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 예제 1: 단변수 함수의 경사하강법 구현\n",
        "\n",
        "**문제:**\n",
        "함수  $f(x) = x^2 - 4x + 3$  의 최소값을 경사하강법으로 찾아보자.\n",
        "\n",
        "**힌트:**\n",
        "\n",
        "- 도함수  $f’(x) = 2x - 4$ 를 사용한다.\n",
        "- 시작점, 학습률, 반복횟수를 설정한다."
      ],
      "metadata": {
        "id": "mw9BI4H4mSaB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wtZf4lZmJis",
        "outputId": "2da86b4a-3b13-4569-bbb0-08749a254748"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "최소점 x: 2.0000, 최소값: -1.0000\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def f(x):\n",
        "  return x**2 - 4*x +3\n",
        "\n",
        "def grad_f(x):\n",
        "  return 2**x -4\n",
        "\n",
        "x = 0.0\n",
        "lr = 0.1\n",
        "for i in range(100):\n",
        "  x -= lr*grad_f(x)\n",
        "print(f\"최소점 x: {x:.4f}, 최소값: {f(x):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 예제 2: 학습률 변화에 따른 수렴 속도 비교\n",
        "\n",
        "**문제:**\n",
        "학습률을 0.01, 0.1, 0.5로 바꿔가며 반복 횟수 50에서  $x$ 의 값을 비교하라."
      ],
      "metadata": {
        "id": "M_VUStHIm36q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for lr in [0.01, 0.1, 0.5]:\n",
        "  x=0.0\n",
        "  for i in range(50):\n",
        "    x -= lr*grad_f(x)\n",
        "  print(f\"lr={lr}: x={x:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_mw9Oy7mRay",
        "outputId": "990e122e-22e1-46c6-b67c-5ed64fa2a6f1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lr=0.01: x=1.1975\n",
            "lr=0.1: x=2.0000\n",
            "lr=0.5: x=2.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 예제 3: 2차원 함수의 경사하강법\n",
        "\n",
        "**문제:**\n",
        "함수  $f(x, y) = (x-1)^2 + (y+2)^2$ 의 최소점을 경사하강법으로 찾아라."
      ],
      "metadata": {
        "id": "oj5k6PGynLYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grad_fx(x, y):\n",
        "  return 2*(x-1)\n",
        "def grad_fy(x, y):\n",
        "  return 2*(y+2)\n",
        "\n",
        "x, y = 0.0, 0.0\n",
        "lr = 0.1\n",
        "for i in range(100):\n",
        "  x -= lr*grad_fx(x, y)\n",
        "  y -= lr*grad_fy(x, y)\n",
        "print(f\"최소점: x={x:.4f}, y={y:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQyrb6RCnLvK",
        "outputId": "43213bda-5fbf-48bf-d710-1f82021a88fd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "최소점: x=1.0000, y=-2.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 예제 4: 확률적 경사하강법(SGD) 기본 구현\n",
        "\n",
        "**문제:**\n",
        "임의의 1차 데이터에 대해 SGD로 선형 회귀 파라미터를 추정하라."
      ],
      "metadata": {
        "id": "u_nHZgH-ntel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "X = np.random.rand(100, 1)\n",
        "y = 3*X[:,0] + 2 + np.random.randn(100) * 0.1\n",
        "\n",
        "w, b = 0.0, 0.0\n",
        "lr = 0.1\n",
        "for epoch in range(10):\n",
        "  for i in range(len(X)):\n",
        "    xi, yi = X[i, 0], y[i]\n",
        "    pred = w*xi + b\n",
        "    grad_w = 2*(pred-yi)*xi\n",
        "    grad_b = 2*(pred-yi)\n",
        "    w -= lr*grad_w\n",
        "    b -= lr*grad_b\n",
        "\n",
        "print(f\"추정된 w: {w:.4f}, b:{b:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajmpkTkLnuD7",
        "outputId": "244db9e4-6fd6-4562-8674-57802a8dad35"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "추정된 w: 2.9375, b:1.9987\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 예제 5: Momentum을 적용한 경사하강법\n",
        "\n",
        "**문제:**\n",
        "Momentum 기법을 적용해 예제 1의 함수의 최소점을 찾아라."
      ],
      "metadata": {
        "id": "gCSCyFcUodmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = 0.0\n",
        "v = 0.0\n",
        "lr = 0.1\n",
        "momentum = 0.9\n",
        "for i in range(100):\n",
        "  grad = grad_f(x)\n",
        "  v = momentum * v - lr * grad\n",
        "  x += v\n",
        "\n",
        "print(f\"Momentum 적용 최소점 x: {x:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "828LuSbQodzk",
        "outputId": "dc756d44-ea5f-4261-9618-5458a30a8464"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Momentum 적용 최소점 x: 2.0058\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 함수 $ f(x) = x^4 - 3x^2 + x $의 최소값을 경사하강법으로 찾아라."
      ],
      "metadata": {
        "id": "GYu4bgoXo4s-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def f(x):\n",
        "  return x**4 - 3*x**2 + x\n",
        "\n",
        "def grad_f(x):\n",
        "  return 4*x**3 - 6*x + 1\n",
        "\n",
        "x = 0.0\n",
        "lr = 0.1\n",
        "for i in range(1000):\n",
        "  x -= lr * grad_f(x)\n",
        "\n",
        "print(f\"시작점 0에서 최소점 x: {x:.4f}, 최소값: {f(x):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCTFjTXUo6XT",
        "outputId": "cc0e67a3-6090-4af2-a558-27c02580551b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "시작점 0에서 최소점 x: -1.3008, 최소값: -3.5139\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. 학습률 0.001, 반복 500회로 예제 1의 함수 최소점과 값을 구하라."
      ],
      "metadata": {
        "id": "e-0H3QIyppdk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def f(x):\n",
        "  return x**4 - 3*x**2 + x\n",
        "\n",
        "def grad_f(x):\n",
        "  return 4*x**3 - 6*x + 1\n",
        "\n",
        "x = 0.0\n",
        "lr = 0.001\n",
        "for i in range(500):\n",
        "  x -= lr * grad_f(x)\n",
        "\n",
        "print(f\"x: {x:.4f}, f(x): {f(x):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryxdjp9spqx6",
        "outputId": "26202be4-a9e5-4292-fd7f-62f4d0b4e9c9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: -1.2322, f(x): -3.4818\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Numpy의 벡터 연산을 이용해 2차원 함수 $ f(x, y) = x^2 + y^2 $의 최소점을 경사하강법으로 구하라."
      ],
      "metadata": {
        "id": "UeXVvZj9p9Ic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def grad_f(vec):\n",
        "  return 2*vec\n",
        "\n",
        "vec = np.array([3.0, -4.0])\n",
        "lr = 0.1\n",
        "for i in range(100):\n",
        "  vec -= lr * grad_f(vec)\n",
        "\n",
        "print(f\"최소점: {vec}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZlJrhvgp9ck",
        "outputId": "b5685363-2db4-4463-e77f-93072498c1c4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "최소점: [ 6.11110793e-10 -8.14814391e-10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. SGD와 배치 경사하강법(BGD)을 비교하여 설명해보시오."
      ],
      "metadata": {
        "id": "v8Aad8Rcq838"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BGD는 매 스텝에서 전체 데이터를 사용하여 그 기울기를 계산하지만 SGD는 한 스텝에서 하나의 데이터만 사용하여 기울기를 계산한다. 따라서 BGD는 학습 데이터의 크기가 클수록 계산 시간이 길어지지만 최적값에 가까이 접근한다. SGD는 수렴이 빠르지만 최적값에 수렴하지 않을 가능성이 있으며 안정성이 낮다."
      ],
      "metadata": {
        "id": "SyvDxH6VrHIv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. 예제 1 함수에 대해 Adam 최적화 알고리즘을 구현한  다음 코드를 보고, Adam 최적화 알고리즘의 식을 정리해보시오."
      ],
      "metadata": {
        "id": "R1Ya59D3sBgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def f(x):\n",
        "  return x**4 - 3*x**2 + x\n",
        "\n",
        "def grad_f(x):\n",
        "  return 4*x**3 - 6*x + 1\n",
        "\n",
        "m, v = 0, 0\n",
        "beta1, beta2 = 0.9, 0.999\n",
        "eps = 1e-8\n",
        "x = 0.0\n",
        "for t in range(1, 1001):\n",
        "  g = grad_f(x)\n",
        "  m = beta1*m + (1-beta1)*g\n",
        "  v = beta2*v + (1-beta2)*(g**2)\n",
        "  m_hat = m/(1-beta1**t)\n",
        "  v_hat = v/(1-beta2**t)\n",
        "  x -= 0.05 * m_hat/(np.sqrt(v_hat)+eps)\n",
        "\n",
        "print(f\"Adam 최소점: {x:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LUT13sMq_aO",
        "outputId": "f2a4cfb9-b4e0-4d0e-d3ef-ed3d2fa44e7b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adam 최소점: -1.3008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. 학습률 스케줄링(learning rate decay)을 적용해 수렴 속도를 비교하라."
      ],
      "metadata": {
        "id": "vcbHbpVptOZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# 예제 6\n",
        "def f(x):\n",
        "  return x**4 - 3*x**2 + x\n",
        "\n",
        "def grad_f(x):\n",
        "  return 4*x**3 - 6*x + 1\n",
        "\n",
        "x = 0.0\n",
        "for i in range(100):\n",
        "  lr = 0.01 * (0.99 ** i)\n",
        "  x -= lr * grad_f(x)\n",
        "\n",
        "print(\"예제 6의 방식 - 실행 100회\")\n",
        "print(f\"x: {x:.4f} f(x): {f(x):.4f}\")\n",
        "\n",
        "def f(x):\n",
        "  return x**4 - 3*x**2 + x\n",
        "\n",
        "def grad_f(x):\n",
        "  return 4*x**3 - 6*x + 1\n",
        "\n",
        "x = 0.0\n",
        "for i in range(50):\n",
        "  lr = 0.01 * (0.99 ** i)\n",
        "  x -= lr * grad_f(x)\n",
        "\n",
        "print(\"예제 6의 방식 - 실행 50회\")\n",
        "print(f\"x: {x:.4f} f(x): {f(x):.4f}\")\n",
        "\n",
        "# 기존\n",
        "def f(x):\n",
        "  return x**4 - 3*x**2 + x\n",
        "\n",
        "def grad_f(x):\n",
        "  return 4*x**3 - 6*x + 1\n",
        "\n",
        "x = 0.0\n",
        "lr = 0.01\n",
        "for i in range(100):\n",
        "  x -= lr * grad_f(x)\n",
        "\n",
        "print(\"기존 방식 - 실행 100회\")\n",
        "print(f\"x: {x:.4f}, f(x): {f(x):.4f}\")\n",
        "\n",
        "def f(x):\n",
        "  return x**4 - 3*x**2 + x\n",
        "\n",
        "def grad_f(x):\n",
        "  return 4*x**3 - 6*x + 1\n",
        "\n",
        "x = 0.0\n",
        "lr = 0.01\n",
        "for i in range(100):\n",
        "  x -= lr * grad_f(x)\n",
        "\n",
        "print(\"기존 방식 - 실행 50회\")\n",
        "print(f\"x: {x:.4f}, f(x): {f(x):.4f}\")\n",
        "\n",
        "# 예제 6의 방식은 시행마다 학습률이 감소하여 기존 방식보다 수렴 속도가 느리다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kc4rGRNSuHQK",
        "outputId": "dd01c033-4f86-46ce-e785-c6b6a7cb802c"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "예제 6의 방식 - 실행 100회\n",
            "x: -1.2904 f(x): -3.5131\n",
            "예제 6의 방식 - 실행 50회\n",
            "x: -1.0411 f(x): -3.1179\n",
            "기존 방식 - 실행 100회\n",
            "x: -1.3008, f(x): -3.5139\n",
            "기존 방식 - 실행 50회\n",
            "x: -1.3008, f(x): -3.5139\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. 특성 스케일링 여부가 경사하강법 수렴에 미치는 영향을 설명해보시오."
      ],
      "metadata": {
        "id": "X8cusQZju9Ne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "특성 간의 스케일 차이가 크면 수렴 속도가 느려질 수 있다. 표준화, 정규화, Min-Max 스케일링 등을 활용하면 수렴 속도가 더 빨라질 것이다.\n",
        "스케일링을 하면 경사하강법이 빠르게 수렴"
      ],
      "metadata": {
        "id": "cc10NfRryk41"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. 임의의 3차 함수에서 여러 시작점으로 경사하강법을 실행하고 결과를 비교하라."
      ],
      "metadata": {
        "id": "ThII9-91zH3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def f(x):\n",
        "  return (x-8)*(x-2)*(x+7)\n",
        "\n",
        "def grad_f(x):\n",
        "  return 3*x**2 - 6*x -54\n",
        "\n",
        "for start in [-2, 0, 2, 4]:\n",
        "  x = start\n",
        "  for i in range(100):\n",
        "    x -= 0.1 * grad_f(x)\n",
        "  print(f\"시작점 {start}에서 최소점 x:{x:4f}, 최소값:{f(x):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DhKylxE09nj",
        "outputId": "93951fb3-9f5b-43c9-8483-0a6f570ed832"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "시작점 -2에서 최소점 x:7.005698, 최소값:-69.7088\n",
            "시작점 0에서 최소점 x:1.028977, 최소값:54.3483\n",
            "시작점 2에서 최소점 x:2.702317, 최소값:-36.0989\n",
            "시작점 4에서 최소점 x:1.020784, 최소값:54.8153\n"
          ]
        }
      ]
    }
  ]
}